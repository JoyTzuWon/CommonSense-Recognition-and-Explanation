# train_setting.py
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=7,
    learning_rate=1e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=4,
    warmup_steps=250,
    lr_scheduler_type="constant",
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=50,
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=1,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    greater_is_better=True,
    seed=2025,
    fp16=True,
)

# training_args = TrainingArguments(
#     output_dir="./results",
#     num_train_epochs=5,
#     learning_rate=1e-5,
#     per_device_train_batch_size=2,
#     per_device_eval_batch_size=2,
#     gradient_accumulation_steps=32,
#     warmup_steps=250,
#     lr_scheduler_type="constant",
#     weight_decay=0.01,
#     logging_dir="./logs",
#     logging_steps=20,
#     evaluation_strategy="epoch",
#     save_strategy="epoch",
#     save_total_limit=1,
#     load_best_model_at_end=True,
#     metric_for_best_model="accuracy",
#     greater_is_better=True,
#     seed=2025,
#     fp16=True,
# )

# training_args = TrainingArguments(
#     output_dir="./results",
#     num_train_epochs=5,
#     learning_rate=3e-5,
#     per_device_train_batch_size=2,
#     per_device_eval_batch_size=2,
#     gradient_accumulation_steps=32,
#     warmup_steps=250,
#     lr_scheduler_type="linear",
#     weight_decay=0.01,
#     logging_dir="./logs",
#     logging_steps=20,
#     evaluation_strategy="epoch",
#     save_strategy="epoch",
#     save_total_limit=1,
#     load_best_model_at_end=True,
#     metric_for_best_model="accuracy",
#     greater_is_better=True,
#     seed=2025,
#     fp16=True,
#     max_grad_norm=1.0,
# )

